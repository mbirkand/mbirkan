<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Deciphering Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" class="alt style2">
						<a href="index.html" class="logo"><strong>HOME</strong> <span></span></a>
					<!--	<nav>
							<a href="#menu">Menu</a>
						</nav>
					-->
					</header>

				<!-- Menu -->
				<!--	<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="landing.html">Landing</a></li>
							<li><a href="about.html">About</a></li>
							<li><a href="elements.html">Elements</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>
				-->
				
				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="images/pic07.jpg" alt="" />
							</span>
							<header class="major">
								<h1>Deciphering Big Data</h1>
							</header>
							<div class="content">
								<p>August 2023 Section</p>
							</div>
						</div>
					</section>

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h2>Module ePortfolio Learning Activities</h2>
										
									</header>
									<h4>Unit 2</h4>
									
									<h5>Internet of Things (IoT)</h5>
									
									<p>Internet of things has emerged as a revolutionary concept which may enable the opportunity to transform multiple industries which eventually enable multiple devices to collect data and more importantly exchange data. 

Huxley et al (2020) discusses about the data wrangling processes such as data extraction, cleaning, formatting and preparation for analysis.  In an era where the Internet of Things concept has gained widespread acceptance, it automatically implies an increase in data volumes necessitating scraping, extraction, cleansing, and comprehensive analysis. Opportunities of IoT is the efficiency and convenience, devices have the ability to enhance the convenience in many sectors including but not limited to smart homes to healthcare systems. It will enable the to gain data driven insights and foster the innovation. Yet that brings limitations as well such as security concerns. Due to its nature most of the IoT devices are vulnerable to cyber attacks and this may lead to data breaches, which also raises the issue of privacy for its users. Another challenge can be standardization as different manufacturers often use different protocols. Combining all these may lead to scalability issues about the data volumes and managing data and processing data can become a significant challenge.</p> 

<p>References: </p>
<p>Huxley et al (2020) Big data architectures - Azure Architecture Center. [online] learn.microsoft.com. Available at: https://learn.microsoft.com/en-us/azure/architecture/data-guide/big-data/.</p>

									<h4>Unit 3</h4>


<p> In this chapter the aim was to write a Python code to scrape web for useful information. I used BeautifulSoup library and pandas libraries for that purpose.
Then scraped the blog of Thinkful.com and scraped their latest blog titles. After that scraping, information found was converted to a dataframe and finally parsed into a Json file.</p>
<p> You can download results Json file <a href="/webScrapingResults.json">here</a> </p>
									
************* 
<p> Code starts here</p>
*************
									
<p> from bs4 import BeautifulSoup</p>

<p>import requests</p>
<p>url = "https://www.thinkful.com/blog/tag/data-science/"</p>

<p>page = requests.get(url)</p>

<p>soup = BeautifulSoup(page.text,'html')</p>

<p>allData = soup.find_all('h2')</p>

<p>newText = [title.text.strip() for title in allData]</p>
<p>print(newText)</p>

<code>['Data Science\n\nData Science Interview Questions\n\n\nLearn how to tackle behavioral and technical questions, and ultimately breeze through even the most intense data science interview.', 'Data Science\n\nData Science Podcasts\n\n\nIf you’re interested in staying up to date with the latest developments in the field, a data science podcast might be the perfect way to catch the latest.', 'Data Science\n\nData Science Tools\n\n\nA big part of becoming a data scientist is understanding how to utilize these tools meaningfully in your role.', 'Data Science\n\nLearn Data Science\n\n\nThis fast-growing discipline helps companies to analyze raw data, and provides valuable information about consumers, the market, and employees.', 'Data Science\n\nData Science School\n\n\nWe took a broad look at the opportunities out there for data scientists, along with an overview of the key skills and concepts you’ll need to master on your mission to becoming a data expert.', 'Data Science\n\nData Science Jobs\n\n\nWe’ll list a few job search platforms and take you through some data scientist interview questions to help you get hired as a data scientist.', 
	'Data Science\n\nData Scientist Resume\n\n\nIf you’ve already got the skills you need to get hired, we’re here to help you through the next step: crafting the perfect data science resume.', 'Data Science\n\nData Science Skills\n\n\nIn this article, we’ll discuss what you need to learn to become a data scientist.', 'Data Science\n\nData Science Training\n\n\nWe’ve outlined some of the skills you need to learn as well as your course options to help you start your new career.', 'Data Science\n\nEntry Level Jobs in Data Science\n\n\nIf you’re considering putting your programming skills and analytical mindset to work, let’s walk through the steps of securing an entry-level data science position.', 'Data Science\n\nHow to Get A Job in Data Science\n\n\nThis article looks at the role of a data scientist, the different specializations within the field, and the skills needed to get hired.', 'Data Science\n\nAre Data Science Degrees Worth It?\n\n\nSelecting the right program for your specific needs can be a tough decision, so we’re to help you consider your options.', 'Data Science\n\nData Science Internships\n\n\nIf you have a passion for big data and are looking to kickstart your career in data science, read on.', 'Data Science\n\nData Science Certificates\n\n\nCertifications validate your skills, which further increases your chances of getting hired. Here we  list the top certification programs to help you thrive as a data scientist.', 'Data Science\n\nData Science Cover Letter\n\n\nA cover letter is sometimes even more important than a resume. So we’re going to guide you through the process of creating a strong data science cover letter.']</code>

<p>import pandas as pd</p>

<p>df=pd.DataFrame(newText)</p>

<p>df.to_json(r'/Users/birkandurak/Desktop/webScrapingResults.json')</p>

*************
<p> Code ends here</p>
*************
								



									<h4>Unit 4</h4>
									<p>In this unit, the focus was on the Data cleaning. Data cleaning is an essential pre-processing step in data analysis. Data management pipeline has 6 mains steps;
<ul>

  <li>Capturing raw data  </li>
  <li>Data cleaning </li>
  <li>Data integration </li>
  <li>Database design </li>
  <li>Data analysis </li>
  <li>Data presentation/visualization</li></ul>
The steps begins with capturing the raw data yet as raw data often comes with imperfections, inaccuracies or inconsistencies, cleaning the data become inevitable. Later on all the data gathered must be converted into a unified view, harmonized into one. This integrated data is then structured through effective database design, organizing it in a way that facilitates efficient retrieval and analysis. The heart of the process lies in data analysis, where sophisticated techniques and algorithms are applied to uncover patterns, correlations, and trends within the data. Finally the insights derived from the analysis are presented visually in an understandable narrative. </p>

									<h4>Unit 5</h4>
									<p>In Unit 5, I have used xlrd library to read the excel file of Unicef and Pandas library to transform the data of Unicef to derive meaningful results.</p>

									<h4>Unit 6</h4>
									<p>
Here is our group project report for our first assignment. 


							<p>Development Team Project: Project Report – Database Design </p>

<p>Richard Charnock, Birkan Durak and Lucy Ringwood </p>

<h4>Objective</h4>
<p>A UK rail company has commissioned a new database system to track contact from customers through to resolution and allow data analysis to help identify areas requiring service improvement. It is intended to record contact from various communication methods including direct customer input into the company’s website will require validation and cleaning. Open-source data feeds from National Rail should be integrated to aid reliable categorization of the data. The system is expected to replace and incorporate a spreadsheet currently used to log contact.
<h4>Logical Design</h4> 
A database solution will collect data and store it in a highly structured format, making it accessible for users managing cases or searching the system, aiding data retrieval for analysis (Bookshear & Brylow, 2020). The database is intended to collect data on public contact flowing into the organisation and store an end-to-end view of the type and subject; organisational ownership of contact; and outcomes/ resolutions of the contact.
The system will hold data in a structured format but the inputs into the system come from multiple sources in different formats. Data directly from National Rail regarding train services and stations is available in semi-structured machine-readable XML format. The legacy customer service data is held in an XLSX, a highly human-readable format that is not easily read by database applications and requires processing before being usable. The data from other internal organisational datasets, such as HR systems, is retrievable in CSV format and considered semi-structured because it does not have the same level of organisation as a database system (EMCS 2015; Olhurst 2012)
The system is intended to follow best practice normal form rules of database design. This involves storing data in related groups per table to avoid duplicating rows within tables and repeating data across the system. The approach improves data consistency; avoids storing redundant data; and minimises insertion, deletion and modification errors as changes are made in one location (Harrington, 2009). The tables in the database each contain IDs, which operate as primary keys allowing association with related tables to join data across the system (Kazil & Jurmul, 2016). 
</p>

<img src="1.png" alt="1" width="500" height="333">
									
<p>The system will hold various data types within each table. IDs will be unique integer numbers. Varchar fields can hold strings of characters of various lengths, this will allow direct text entries as well as descriptive strings relating to data values to make them human-readable. Date Time fields record when key events occurred. (Harrington, 2009).								
</p>


<img src="2.png" alt="2" width="500" height="333">									
<h4>Data management Pipeline</h4>
<p>Creating a robust data management pipeline is crucial precise handling of customer interactions and upholding customer satisfaction levels.
At the data-capturing stage, a Representational State Transfer Application Programming Interface (REST API) can be used to access data from the National Rail public data feed. Data retrieval will be executed in Python, utilising the “requests” library to initiate an HTTP POST and GET requests to access required data. This data is saved in XML format for cleaning and processing (Wiki.openraildata.com, 2023). As National Rail's data feed is a trusted source, data is expected to be standardised and inherently reliable in terms of structural integrity. Additional sources to be integrated into the database, such as customer-initiated input and legacy data from spreadsheets, are likely to be unstructured and varied in data types.
An initial step in the data-cleaning phase is to transform the data into a format suitable for database insertion. After identifying headers to establish a foundational structure, the “ElementTree” library can be used to parse the data. A loop can be used to leverage these headers to segregate the data, facilitating creation of a list of dictionaries to create a Pandas dataframe (Pandey, 2021). This step also aids in implementing data-mapping techniques, ensuring accurate matching between primary and foreign keys, particularly when merging tables from diverse and formats – XML, XLSX, CSV. 
Regarding anomalies, null values are most likely to be frequently encountered. While transforming XML data, the loop can also be used to replace blank values with a designated placeholder—typically "none"—to signify absence of content.
Another critical process is standardisation of dates/ times. For this purpose, the Python “datetime” library can be implemented (Kazil and Jarmul, 2016). All datetime data should be converted into the same format, e.g. ISO 8601 (PyNative, 2022). Standardising timestamps is vital for maintaining data integrity, ensuring subsequent analyses are accurate, thereby guiding decision-making more effectively.
</p>

<h4>Database Build</h4>
<p>In the landscape of a data-driven world, designing and implementing an efficient database system is vital. The database model should address the challenges such as storage, user access and data manipulation, therefore our goal is to produce a scalable and well-structured database model which can serve the organization’s needs. The mission statement for the database is to monitor complaints originating from different channels which will allow the company to enhance its services and acquire valuable insights.</p>									
<img src="3.png" alt="1" width="500" height="333">

									
<p>View integration approach can be used to manage multiple user views, acknowledging not all users need to manipulate the data. </p>
<p>The following should be considered (Connolly et al., 2014): </p>
<p>Usability: Ensuring user friendliness for both technical and non-technical staff.</p>
<p>Performance and functionality: Automating tasks, filtering, forecasting and visualizing should be possible without performance issues. Integration with legacy spreadsheets is important.</p>
<p>Scalability: Growing user base should be considered.</p>
<p>Networking and shared access information: Supporting concurrent access for at least 5 analysts.</p>
<p>Security: Prioritizing secure storage of sensitive data. </p>
<p>Backup and recovery: Implementing daily automatic backups.</p>
<p>Legal issues: System must adhere to legal regulations.</p>
<p>Visualization and reporting: Transforming data into insightful reports.</p>
<p>Cost: Cost should align with functionality.</p>

Considering all the terms of references, Amazon RDS (Relationship Database Service) emerges as the prominent database management system. It stands out for its user-friendly interface, flexibility, scalability and cost-effectiveness, offering the advantage of “paying on demand” for utilized resources. Moreover, it facilitates easy access to legacy databases such as mySql or Oracle. Therefore users will have a chance to process the substantial data workloads in a single unified database.
</p>

<h4>Conclusion</h4>									


<p>The new database system aims to revolutionise customer interaction tracking and service improvement. Utilising best practices, it integrates data from various sources—National Rail feeds, legacy spreadsheets, and other organisational datasets—while addressing crucial aspects like usability, scalability, and security. The data management pipeline ensures accurate and efficient handling of the data, transforming disparate formats into a relational database. Opting for Amazon RDS as the database management system aligns with the company's needs for a user-friendly, scalable, and cost-effective solution. This comprehensive approach enables the company to enhance its customer service significantly and make data-driven decisions.</p>
									
<h4>References</h4>
<p>Brookshear, J. G. & Brylow, D. (2020) Computer science : an overview. Thirteenth, global edition. New York, New York: Pearson.</p>
<p>EMCS. (2015) Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data. Wiley Professional Development. Available via VitalSource Bookshelf [Accessed 17 September 2023]</p>
<p>Harrington, J. L. (2009) Relational database design and implementation clearly explained. 3rd ed. Amsterdam : Morgan Kaufmann/Elsevier.</p>
<p>Kazil, J. & Jarmul, K. (2016) Data Wrangling with Python. 1st ed. Sebastopol, CA: O’Reilly.</p>
<p>National Rail (2023) National Rail data feeds. Available at: https://www.nationalrail.co.uk/developers/ [Accessed 17 September 2023] </p>
<p>Ohlhorst, F.J. (2012) Big Data Analytics: Turning Big Data into Big Money. Wiley Professional Development.</p>
<p>Wiki.openraildata.com. (2023) KnowledgeBase. Available from: https://wiki.openraildata.com/KnowledgeBase [Accessed 16 September 2023)</p>
<p>Pandey, P (January 19, 2021) Extracting information from XML files into a Pandas dataframe, Towards Data Science. Available from: https://medium.com/p/11f32883ce45 [Accessed 16 September 2023).</p>
<p>PyNative.com (2022), Python ISO 8601 Datetime, Available from: https://pynative.com/python-iso-8601-datetime/ [Accessed 17 September 2023)</p>
<p>Connolly, Thomas, et al. (2014) Database Systems: a Practical Approach to Design, Implementation, and Management, Global Edition, Pearson Education, Limited, 2014. ProQuest Ebook Central, [Accessed 17 September 2023]</p>
							
									
									
									</p>



									<h4>Unit 12</h4>
<p>Final version of the team project can be found below;</p>

<h4> Development Team Project: Project Report – Executive Summary </h4>
 
<p>Richard Charnock, Birkan Durak and Lucy Ringwood </p>

<h4>Introduction </h4>

<p>This report provides an executive summary of the completed design and build of the customer contact management database for a UK rail company.</p>

<h4>Data Collection Methods </h4>
 
<p>There are 4 different input types for the database which are; Data forms (via web), Legacy data (spreadsheets), Data feed of National Rail Enquires and Data feed Internal HR Systems.  
 
Application Programming Interfaces (APIs) are mechanisms for computer programs to communicate with each other (Connoly, 2015). To ensure data integration in a standardized format, utilizing APIs is crucial. Amazons REST API proves to be the more suitable option for 4 main reasons compared to its competitors. First, it is cost-effective, and it will seamlessly connect with the data of the customer and with the database, in a pre-formatted data format eliminating the need for data cleaning.  After integration, AWS provides real-time monitoring services and as security is a crucial consideration, given the sensitive nature of the information being transmitted, Amazon has ISO and CSA Star certifications. 
 
Therefore, employing Amazon API Gateway as a unified solution is a reliable approach with seamless integration into its database. Figure 1. summarizes how Amazon API Gateway will be utilized between the database and data inputs.  </p>

<img src="4.png" alt="4" width="500" height="333">									
<img src="5.png" alt="5" width="500" height="333">	
									

<h4>Database Management System Selection </h4>
 
<p>Using an efficient database is the core and our goal is to produce a well-structured and scalable database model. Model should address the challenges of storage, user access and data manipulation. Its mission is to oversee complaints from various sources, enabling the company to improve its services and gain valuable insights. </p>
		
<img src="6.png" alt="5" width="500" height="333">

									
<p>When the data has a clear structure and defined relationships between different entities, SQL databases, which are relational databases, are designed to handle such data efficiently. They use tables to store data and allow complex queries and transactions. Also SQL databases support complex queries making them better partners for data manipulation (Connolly, 2014). Therefore, given the nature of our dataset, SQL solutions are more suitable for our purposes. When evaluating database options like Amazon RDS, MySQL, and MongoDB, it's essential to consider the specific limitations and advantages of each system. MySQL, despite its open-source aspect, faces constraints due to Oracle's licensing, limiting its capabilities and lacking support for certain SQL frameworks. It was primarily designed for small to medium-sized web-based solutions with limited data involvement therefore, scalability is limited which can be a huge obstacle. MongoDB, while versatile, presents challenges. Its complexity in interpreting other query languages and its original design not optimized for relational data models can lead to performance slowdowns, especially in cases requiring relational data handling.  
 
Considering all, Amazon RDS (Relational Database Service) emerges as the leading database management system with its ability to overcome the limitations of MySql and MongoDb. Its standout features include a user-friendly interface, flexibility, scalability, and cost-effectiveness, with the added benefit of "pay-as-you-go" pricing. Additionally, it allows effortless integration with SQL. Consequently, users have the opportunity to handle significant data workloads within a unified database environment. 
 
Implementing a view integration approach for users is crucial. It simplifies access to complex data structures, ensuring users engage with the database only as necessary. This approach enhances security by enabling customizable access levels, enhancing overall data security. Furthermore, it optimizes performance by efficiently retrieving results, leading to faster query response times (Connolly, 2014). Major views table can be seen below which summarizes that customers will only have ability to do queries. Analysts will work on maintaining and reporting and managers will work only on maintaining part.  
</p>									

									
<img src="7.png" alt="5" width="500" height="333">									
									
<h4>Data Wrangling </h4>

<p>As this database contains existing data and has been implemented to facilitate expansion from new reporting mechanisms, an increasing volume of data can be expected to grow within it over time. Efficient data wrangling is a paramount phase in the data management lifecycle to ensure a more well-organised, valuable and memory-efficient format (McKinney, 2017). 

Amazon's RDS provides a scalable and reliable database solution, but data arriving in different formats may lead to potential discrepancies in the database (AWS, n.d). To mitigate this, AWS Glue has been utilised for ETL (Extract, Transform, Load) processes to perform data preparation. While Glue can generate Python code for these processes, "auto-generated" code can be convoluted and less efficient than hand-written scripts, leading to increased execution time for large datasets (AWS Workshops (n.d.).

Key transformation stages:
</p>								


<ul>
  <li>Cleaning - handling missing values, removing duplicates and correcting inconsistencies (for example, postcode format when entered manually in the customer input forms). This stage ensures that the data is all standardised and the content of different datasets is managed consistently</li>
  <li>Enrichment - adding industry-specific data to the dataset such as the regional office based on the customer’s location, priority codes based on the severity of the complaint and allocated team based on the complaint category (service issue/staff issue/ repairs and maintenance). </li>
  <li>Transformation - structuring the data to align with the database schema.  </li>
  <li>Formatting - ensuring all datatypes are consistent throughout, for example using the Python Datetime library to standardise all temporal data.  </li>
</ul>

<p>These tasks have been completed on the initial dataset upload but will also require execution on incoming data. In this instance, AWS Lambda allows for event-driven execution (Patterson, 2019). For example, when a daily upload of data is available in an S3 bucket, the Lambda function triggers the specified Glue job. This benefits the organisation as Lambda functions are a “pay-for-what-you-use” service and therefore costs are lower than running a server (Patterson, 2019).  

A drawback of Lambda functions is that they have a maximum execution time of 15 minutes, so are not suited to extensive data processing. For scenarios such as annual strategic analysis reports, the AWS Batch tool will facilitate the processing of large volumes of data without needing to consider time to execute and computing resources (AWS, n.d). 
</p>								
									
	
<h4>Data structure and analysis</h4>
<p>The new customer service platform collects data that is primarily structured data in the form of constrained text values, dates and some longer free text fields, different parts of the process form logical units such as customer information, involved staff and train services. Data processes with links and connections are very suited to being stored a SQL relational database (Kazil & Jarmul, 2016), therefore the system has been built as a Relational Database Management System (RDBMS) using SQL on the Amazon RDS service (AWS Amazon 2023). SQL based RDMS are a highly structured way of storing data in a manner which results in good data integrity and consistency. SQL allows fast access to data; is considered reliable and stable; can handle large volumes of data; and can scale as the size of data or variety of data grows (Welling & Thomson, 2004). 
Data in the system has been grouped into relations (tables), with each containing attributes (columns) with an identified data type. Each tuple (row) within a table represents a distinct value that records part of the process and has a unique ID. This is used as an identifier for that row and as a primary key to create links in the database between connected relations (Harrington, 2009). Figure 2 below shows examples for two relations:
</p>

<img src="8.png" alt="8" width="500" height="333">										
									
<p>The database design principles of normal forms have been applied at the design and then build stage. Normalisation is intended to organise the system into tables which make logical sense and avoid duplicating data by storing the same data in many tables. (Chapple, 2022) This has the advantage of reducing storage use and improving data integrity. Updates to data need only occur in one table and will apply across the system. If the data was not normalised, an update to a data item such as a phone number in one location would not cascade across the system and member of staff may call the wrong person. Whilst Normalisation is best practice for RDMS, it would not be required for an object orientated solution such as NoSQL (Chapple, 2020) because the data is unstructured. Figure 3 below shows normalisation database design:</p>									
									
<img src="9.png" alt="8" width="500" height="333">		


<p>Database Definition Language (DDL) is the element of SQL that allowed the normalised build of the database to be implemented. DDL allowed tables to be created; names and data types of attributes to be defined; column constraint rules to be set; and primary and foreign keys specified. (Harrington, 2009; Sarkar & Roychowdhury, 2019). This coding process is critical for integrity and data quality rules; it ensures the database links together and provides for the basic structure of the system. Figure 4 shows the creation of two tables:</p>

<img src="10.png" alt="8" width="500" height="333">	


<p>The new system is intended to enable new data insight about the customer experience in order to drive improvements to the delivery of the service. SQL ensures key data is held in logical structure with data redundancy and data quality issues designed out which aids data analysis by providing more reliable data. SQL uses Data Query Language (DQL) to allow flexible querying of the database to retrieve data to answer business questions. DQL allows bespoke data retrieval by joins between tables, aggregation and selection of specific data items. This allows ad-hoc queries to meet business needs or to create temporary views of the database to allow for regular reporting that can be displayed to users as part of the application (Connolly & Begg, 2014). Figure 5 is SELECT query which provides a list of open contact cases by department with customer details:
</p>

<img src="11.png" alt="8" width="500" height="333">	


<p>Python can be used to connect directly to a RDBMS, this provides an option to undertake advanced analysis using a coding language which is highly flexible with libraries designed for Data Science tasks such as data wrangling, statistical analysis and data visualisation (Javin, 2021; Kazil & Jarmul, 2016). The RDMS is compatible with commercial data analysis software, for example AWS offer a suite of data analysis, visualisation and machine learning options (Amazon AWS, 2023). These packages often suitable for less technical users by enabling data analysis without coding, a good option for the organisations in a market where data science skills are highly sought after and not always readily available (World Economic Forum, 2020).
</p>
									
<h4>Compliance</h4> 

<p>With the organisation processing significant amounts of structured data, including personal information, compliance with the UK GDPR is non-negotiable. AWS' Identity and Access Management (IAM) is a pivotal tool, facilitating the granular control of user-access within the AWS environment and enables the principle of least privilege, where users are only given access necessary to perform their tasks (AWS, 2022). This thereby aligns with GDPR's requirements for robust access management (ICO, n.d).  

The IAM Access Analyser provides insights into AWS resource permissions, helping ensure that only necessary and intended access is granted. Regularly reviewing IAM roles, policies, and permissions can align with the periodic access review requirement of ISO/IEC 27001 (ISO 27001 Guide, 2016). 

Data has been stored in a way that allows for easy retrieval or deletion upon a user's request, in line with UK GDPR's access, rectification and erasure provisions (ICO, 2023). The nature of a relational database and having data stored in normal form means that records can be reliably reviewed and modified in the one table they are stored in and have the changes applied across the system from this one action.  
</p>
<h4>Conclusion </h4>

<p>In light of the benefits and challenges discussed, the solution lies in a balanced approach. For instance, while AWS Glue can handle most ETL tasks, for highly specialised transformations, custom Python scripts are necessary. Similarly, for larger or long-running data jobs, considering services like AWS Batch over Lambda might be more suitable. 
 
AWS and Python present a robust solution for data wrangling requirements, but it's imperative to understand the underlying challenges and address them proactively. The designed system, built with a clear understanding of its limitations and strengths, can efficiently support the goals of enhanced customer complaint management. 

A relational database built with SQL is highly suited to the collection of the type of structured data in a logical process collected as part of this solution. Alternative options using NoSQL would be an option if the collection included unstructured data such as images, or if the volume of data had far greater (Keita, 2022), the priority of collecting data in a structured format made a SQL appropriate. This approach may restrict database scalability in the future to handle big data which is more varied, for example incorporating images or high volume and velocity data from Internet of Things (IoT) devices.  
A SQL RDMS provides a range of data analysis options which can scale based on the technical skills and needs of the organisation. Direct SQL queries are highly flexible for retrieving data. Connectivity with Python opens up the possibility of applying advanced analytics techniques. RDMS are also suited to commercial data analysis software which provide capability for staff of varied technical ability. This meets the requirement to enable far more data insight than would have been possible under the legacy platform.
</p>
									

<h4>References </h4>

<p>AWS. (n.d). What Is AWS Glue?. Amazon Web Services, Inc. Available at: https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html [Accessed 21 October 2023].  </p>

<p>AWS. (n.d). Batch-based architecture. In: AWS Well-Architected. Available at: https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/batch-based-architecture.html [Accessed 21 October 2023].  </p>

<p>AWS Amazon (2023). What is a Relational Database? Available from: https://aws.amazon.com/rds/aurora/ [Accessed 20 October 2023]  </p>
<p>AWS Amazon (2023). Amazon Aurora Unparalleled high performance and availability at global scale with full MySQL and PostgreSQL compatibility. Available from: https://aws.amazon.com/relational-database/ [Accessed 20 October 2023]  </p>
<p>AWS Amazon (2023). Analytics on AWS. Fastest way to get answers from all your data to all your users. Available from: https://aws.amazon.com/big-data/datalakes-and-analytics/ [Accessed 20 October 2023]  </p>
<p>AWS (2022) Security best practices in IAM. Available at: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege. [Accessed 21 October 2023].  </p>

<p>AWS Lambda, Amazon API Gateway, and services from Amazon Web Services. 1st edition. Birmingham, England ;: Packt.  </p>

<p>AWS Workshops (n.d). AWS Glue Immersion day. Available at: https://catalog.us-east-1.prod.workshops.aws/workshops/ee59d21b-4cb8-4b3d-a629-24537cf37bb5/en-US/lab6/etl/advanced-transform [Accessed 21 October 2023].  </p>
<p>Patterson, S. (2019) Learn AWS serverless computing : a beginner’s guide to using  </p>

<p>Chapple, M.(2022) Database Normalization Basics, Available from: https://www.lifewire.com/database-normalization-basics-1019735 [Accessed 20 October 2023]  </p>
<p>Chapple, M. (2023) Should I Normalize My Database? Available from: https://www.lifewire.com/should-i-normalize-my-database-1019730 [Accessed 20 October 2023]  </p>
<p>Connolly, T. M. & Begg, C. E. (2014) Database Systems: A Practical Approach to Design, Implementation and Management. 6th ed. Harlow: Pearson Education Limited.  </p>
<p>Harrington, J. L. (2009) Relational database design and implementation clearly explained. 3rd ed. Amsterdam: Morgan Kaufmann/Elsevier.  </p>
<p>ICO (n.d) A guide to data security. Available at: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-to-data-security/#14. [Accessed 21 October 2023].  </p>

<p>ISO 27001 Guide (2016) ISO 27001 Review User Access Rights Requirements. Available at: https://iso27001guide.com/iso-27001-review-user-access-rights-requirements-iso27001-guide-iso27001-guide.html [Accessed 21 October 2023].  </p>

<p>ICO (2023) A guide to individual rights. Available at: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/individual-rights/ [Accessed 21 October 2023]. </p>

<p>Javin.P. (2021) Top 5 Reasons to Learn Python for Data Science, Machine Learning, and AI in 2023?  Available at: https://medium.com/javarevisited/5-reasons-to-learn-python-for-data-science-16a9d4c44d6d [Accessed 20 October 2023]  </p>
<p>Kazil, J. & Jarmul, K. (2016). Data Wrangling with Python. 1st ed. Sebastopol, CA: O’Reilly.   </p>
<p>Keita, Z. (2022) NoSQL Databases: What Every Data Scientist Needs to Know. Available at: https://www.datacamp.com/blog/nosql-databases-what-every-data-scientist-needs-to-know [Accessed 21 October 2023]  </p>
<p>McKinney, W. (2017). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython (2nd ed.). O'Reilly Media.  </p>

<p>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.  </p>
<p>World Economic Forum. (2020) The Future of Jobs Report 2020. Available from: https://www.weforum.org/reports/the-future-of-jobs-report-2020 [Accessed 21 October 2023] </p>

									
									<h2>Individual Contributions</h2>
									<p>In the first group project, first we have discussed about 
									
									
									</p>

									
									<h2>Evaluation of Final Project</h2>
									<p>Demo.</p>									
									
								</div>
							</section>

						<!-- Two ********** SECTION BEGINNING
							<section id="two" class="spotlights">
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic08.jpg" alt="" data-position="center center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Orci maecenas</h3>
											</header>
											<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis tempus.</p>
											<ul class="actions">
												<li><a href="generic.html" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic09.jpg" alt="" data-position="top center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Rhoncus magna</h3>
											</header>
											<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis tempus.</p>
											<ul class="actions">
												<li><a href="generic.html" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>

							*******END OF SECTION*******-->
						
							<!--**********
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic10.jpg" alt="" data-position="25% 25%" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Sed nunc ligula</h3>
											</header>
											<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis tempus.</p>
											<ul class="actions">
												<li><a href="generic.html" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
							</section>

						<!-- Three ****************** -->


							<section id="three">
								<div class="inner">
									<header class="major">
										<h2>Reflections</h2>
									</header>
									<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis libero. Mauris aliquet magna magna sed nunc rhoncus pharetra. Pellentesque condimentum sem. In efficitur ligula tate urna. Maecenas laoreet massa vel lacinia pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis libero. Mauris aliquet magna magna sed nunc rhoncus amet pharetra et feugiat tempus.</p>

							</section>

								</div>

				

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="https://github.com/mbirkand" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/mbirkandurak" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
					
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
